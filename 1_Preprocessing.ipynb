{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 01. Put historical data in df\n",
    "\n",
    "Data were saved as json files, now put them in a single df\n",
    "\n",
    "- Ignore folders only containing a \"_SUCCESS\" file but no data\n",
    "- Remove duplicate rows in df that arise because of getting the same data from arXiv multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, from_json, schema_of_json, regexp_replace, udf\n",
    "from pylatexenc.latex2text import LatexNodes2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = sum(\n",
    "#     1 for name in os.listdir(\"spark/notebooks\")\n",
    "#     if name.startswith(\"saved_data-\") and os.path.isdir(os.path.join(\"spark/notebooks\", name))\n",
    "# )\n",
    "\n",
    "# print(f\"Number of folders starting with 'saved_data': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_json_file(folder_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder_path) if \"SUCCESS\" not in f]\n",
    "\n",
    "        if not files:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_json_file(\"spark/notebooks/saved_data-1743417480000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_json_file(\"spark/notebooks/saved_data-1743416280000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = []\n",
    "\n",
    "for folder in os.listdir(\"spark/notebooks\"):\n",
    "    folder_path = os.path.join(\"spark/notebooks\", folder)\n",
    "    if folder.startswith(\"saved_data\") and os.path.isdir(folder_path):\n",
    "        if has_json_file(folder_path):\n",
    "            data_folders.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_folders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = spark.read.text(data_folders[0]).limit(1).collect()[0][0]\n",
    "first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema_of_json(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.text(data_folders)\n",
    "df_parsed = df_raw.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "df_parsed_deduped = df_parsed.drop_duplicates([\"title\"])\n",
    "\n",
    "df_parsed_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.write.mode(\"overwrite\").parquet(\"data/df_all_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped = spark.read.parquet(\"data/df_all_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.filter(col(\"published\") == \"2025-03-27T09:58:07Z\").select(col(\"summary\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_to_text(s):\n",
    "    return LatexNodes2Text().latex_to_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_to_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    try:\n",
    "        converted = LatexNodes2Text().latex_to_text(s)\n",
    "        return converted.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"^\", \"\")\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_udf = udf(latex_to_text, StringType())\n",
    "\n",
    "df_cleaned = df_parsed_deduped.withColumn(\"title\", latex_udf(col(\"title\"))).withColumn(\"summary\", latex_udf(col(\"summary\")))\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.filter(col(\"published\") == \"2025-03-27T09:58:07Z\").select(col(\"summary\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.write.mode(\"overwrite\").parquet(\"data/df_all_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 02 Scrape arXiv categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/category_taxonomy'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(['h2', 'h3', 'h4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for header in soup.find_all(['h2', 'h3', 'h4']):\n",
    "    if header.name == 'h2':\n",
    "        group_name = header.get_text(strip=True)\n",
    "    elif header.name == 'h3':\n",
    "        archive_name = header.get_text(strip=True)\n",
    "    elif header.name == 'h4':\n",
    "        category_info = header.get_text(strip=True)\n",
    "        # Extract category ID and name\n",
    "        if '(' in category_info and ')' in category_info:\n",
    "            category_id = category_info.split('(')[-1].strip(')')\n",
    "            category_name = category_info.split('(')[0].strip()\n",
    "        else:\n",
    "            category_id = ''\n",
    "            category_name = category_info\n",
    "        categories.append({\n",
    "            'group': group_name,\n",
    "            'archive': archive_name,\n",
    "            'category_id': category_id,\n",
    "            'category_name': category_name\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(categories)\n",
    "df = df[df.group != \"Group Name\"]\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.category_name.str.contains('hep')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/arxiv_categories.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
