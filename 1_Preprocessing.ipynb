{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 01. Put historical data in df\n",
    "\n",
    "Data were saved as json files, now put them in a single df\n",
    "\n",
    "- Ignore folders only containing a \"_SUCCESS\" file but no data\n",
    "- Remove duplicate rows in df that arise because of getting the same data from arXiv multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, from_json, schema_of_json, regexp_replace, udf\n",
    "from pylatexenc.latex2text import LatexNodes2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of folders saved\n",
    "\n",
    "count = sum(\n",
    "    1 for name in os.listdir(\"spark/notebooks\")\n",
    "    if name.startswith(\"saved_data-\") and os.path.isdir(os.path.join(\"spark/notebooks\", name))\n",
    ")\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_json_file(folder_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder_path) if \"SUCCESS\" not in f]\n",
    "\n",
    "        if not files:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_json_file(\"spark/notebooks/saved_data-1743417480000\") # test on folder that only contains _SUCCESS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_json_file(\"spark/notebooks/saved_data-1743416280000\") # test on folder that actually contains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = []\n",
    "\n",
    "for folder in os.listdir(\"spark/notebooks\"):\n",
    "    folder_path = os.path.join(\"spark/notebooks\", folder)\n",
    "    if folder.startswith(\"saved_data\") and os.path.isdir(folder_path):\n",
    "        if has_json_file(folder_path):\n",
    "            data_folders.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_folders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = spark.read.text(data_folders[0]).limit(1).collect()[0][0]\n",
    "first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema_of_json(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.text(data_folders)\n",
    "df_parsed = df_raw.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "df_parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped = df_parsed.drop_duplicates([\"title\"])\n",
    "\n",
    "df_parsed_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.write.mode(\"overwrite\").parquet(\"data/df_all_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped = spark.read.parquet(\"data/df_all_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_to_text(s):\n",
    "    return LatexNodes2Text().latex_to_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_to_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    try:\n",
    "        converted = LatexNodes2Text().latex_to_text(s)\n",
    "        return converted.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"^\", \"\")\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_udf = udf(latex_to_text, StringType())\n",
    "\n",
    "df_cleaned = df_parsed_deduped.withColumn(\"title\", latex_udf(col(\"title\"))).withColumn(\"summary\", latex_udf(col(\"summary\")))\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.filter(col(\"published\") == \"2025-03-27T09:58:07Z\").select(col(\"summary\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.write.mode(\"overwrite\").parquet(\"data/df_all_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 02 Scrape arXiv categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/category_taxonomy'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(['h2', 'h3', 'h4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for header in soup.find_all(['h2', 'h3', 'h4']):\n",
    "    if header.name == 'h2':\n",
    "        group_name = header.get_text(strip=True)\n",
    "    elif header.name == 'h3':\n",
    "        archive_name = header.get_text(strip=True)\n",
    "    elif header.name == 'h4':\n",
    "        category_info = header.get_text(strip=True)\n",
    "        # Extract category ID and name\n",
    "        if '(' in category_info and ')' in category_info:\n",
    "            category_id = category_info.split('(')[-1].strip(')')\n",
    "            category_name = category_info.split('(')[0].strip()\n",
    "        else:\n",
    "            category_id = ''\n",
    "            category_name = category_info\n",
    "        categories.append({\n",
    "            'group': group_name,\n",
    "            'archive': archive_name,\n",
    "            'category_id': category_id,\n",
    "            'category_name': category_name\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(categories)\n",
    "df = df[df.group != \"Group Name\"]\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check High Energy Physics categories - merge them into 1 subcategory?\n",
    "\n",
    "df[df.category_name.str.contains('hep')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/arxiv_categories.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# 03 Create training, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when, collect_set, concat, concat_ws, lit, regexp_replace, rand\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where applicable, split main category (e.g. cs.HC) into level 1 (e.g. cs) and level 2 (e.g. HC) categories\n",
    "\n",
    "df_pub = spark.read.parquet(\"data/df_all_cleaned\")\n",
    "df_pub = df_pub.withColumn(\"level1_category\", split(df_pub[\"main_category\"], \"\\.\")[0]) \\\n",
    "    .withColumn(\"level2_category\", split(df_pub[\"main_category\"], \"\\.\")[1])\n",
    "df_pub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also split arXiv categories into level 1 and level 2, to make it possible to join them with the data\n",
    "\n",
    "df_categories = spark.read.parquet(\"data/arxiv_categories.parquet.gzip\")\n",
    "df_categories = df_categories.withColumn(\"level1_category\", split(df_categories[\"category_name\"], \"\\.\")[0]) \\\n",
    "    .withColumn(\"level2_category\", split(df_categories[\"category_name\"], \"\\.\")[1])\n",
    "df_categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.select('category_name').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with level 1 abbreviation, \"group\" (main categories) and \"subgroup\" (main categories but Physics split up) categories\n",
    "\n",
    "df_level1_categories = df_categories.select(col('group'), col('level1_category'), col('archive')).drop_duplicates().sort(col('group'))\n",
    "df_level1_categories = df_level1_categories.withColumn(\"subgroup\",\n",
    "    when(col(\"level1_category\").isin(\"cs\", \"econ\", \"eess\", \"math\", \"q-bio\", \"q-fin\", \"stat\"), col(\"group\")).otherwise(concat(col(\"group\"), lit(\": \"), col(\"archive\")))\n",
    "    ).drop('archive')\n",
    "df_level1_categories = df_level1_categories.withColumn(\"subgroup\", regexp_replace(col(\"subgroup\"), r\"\\(.*?\\)\", \"\"))\n",
    "df_level1_categories = df_level1_categories.withColumn(\"subgroup\", regexp_replace(\"subgroup\", \"-.*\", \"\")) # merge the 4 High Energy Physics categories\n",
    "\n",
    "df_level1_categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_level1_categories.toPandas().to_parquet(\"data/df_level1_categories.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all subgroups\n",
    "\n",
    "row = df_level1_categories.agg(collect_set(\"subgroup\").alias(\"subgroups\")).collect()[0]\n",
    "\n",
    "print(row['subgroups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categories written out in full (group and subgroup) to the data, based on level 1 category abbreviation (e.g. cs)\n",
    "\n",
    "df_labeled = df_pub.join(df_level1_categories, on = \"level1_category\", how = \"left\")\n",
    "df_labeled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Stratified train / val / test set with the 8 main groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate title and summary; keep only that column and the label (relevant columns)\n",
    "df_relevant = df_labeled.withColumn(\"text\", concat_ws(\". \", col(\"title\"), col(\"summary\"))).select(col('text'), col('group'))\n",
    "df_relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and convert to Pandas df in order to make stratified train, validation, and test set\n",
    "df_shuffled = df_relevant.orderBy(rand()).toPandas().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled.groupby('group')['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_shuffled.groupby('group')['text'].nunique().reset_index(name='n_text')\n",
    "total = df_shuffled['text'].nunique()\n",
    "df_counts['pct'] = df_counts['n_text'] / total\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/validation/test\n",
    "train_df, test_df = train_test_split(df_shuffled, test_size = 0.3, stratify = df_shuffled['group'], random_state=16)\n",
    "val_df, test_df = train_test_split(test_df, test_size = 0.5, stratify = test_df['group'], random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_counts = train_df.groupby('group')['text'].nunique().reset_index(name='n_text')\n",
    "total_train = train_df['text'].nunique()\n",
    "df_train_counts['pct'] = df_train_counts['n_text'] / total_train\n",
    "df_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet(\"data/df_train.parquet.gzip\", index=False)\n",
    "val_df.to_parquet(\"data/df_val.parquet.gzip\", index=False)\n",
    "test_df.to_parquet(\"data/df_test.parquet.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Stratified train / val / test set with subgroups for physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate title and summary; keep only that column and the label\n",
    "df_relevant2 = df_labeled.withColumn(\"text\", concat_ws(\". \", col(\"title\"), col(\"summary\"))).select(col('text'), col('subgroup'))\n",
    "df_relevant2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled2 = df_relevant2.orderBy(rand()).toPandas().reset_index(drop = True)\n",
    "df_shuffled2.groupby('subgroup')['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/validation/test\n",
    "train_df2, test_df2 = train_test_split(df_shuffled2, test_size = 0.3, stratify = df_shuffled2['subgroup'], random_state=16)\n",
    "val_df2, test_df2 = train_test_split(test_df2, test_size = 0.5, stratify = test_df2['subgroup'], random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2.to_parquet(\"data/df_train_17cats.parquet.gzip\", index=False)\n",
    "val_df2.to_parquet(\"data/df_val_17cats.parquet.gzip\", index=False)\n",
    "test_df2.to_parquet(\"data/df_test_17cats.parquet.gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
