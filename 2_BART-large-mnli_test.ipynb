{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.apple.com/metal/pytorch/\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['home', 'earth', 'cat']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"plane tickets are becoming cheaper\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility\"\n",
    "candidate_labels = ['computer science', 'physics', 'economics', 'mathematics']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"We describe a comma 2-comonad on the 2-category whose objects are functors, 1-cell are colax squares and 2-cells are their transformations\"\n",
    "candidate_labels = ['computer science', 'physics', 'economics', 'mathematics']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"We describe a comma 2-comonad on the 2-category whose objects are functors, 1-cell are colax squares and 2-cells are their transformations.  \\\n",
    "We give a complete description of the Eilenberg-Moore 2-category of colax coalgebras, colax morphisms between them and their transformations and we show how many \\\n",
    "fundamental constructions in formal category theory like adjoint triples, distributive laws, comprehension structures, Frobenius functors etc. naturally fit in this context.\"\n",
    "candidate_labels = ['category theory', 'combinatorics', 'commutative algebra', 'algebraic topology']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 02 Test on actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_set, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub = spark.read.parquet(\"data/df_all_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pub.write.option(\"compression\", \"uncompressed\").parquet(\"data/df_all_cleaned_uncompressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories = spark.read.parquet(\"data/arxiv_categories.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_categories.agg(collect_set(\"group\").alias(\"groups\")).collect()[0]\n",
    "\n",
    "categories = row['groups']\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_categories.agg(collect_set(\"category_id\").alias(\"subcategories\")).collect()[0]\n",
    "\n",
    "subcategories = row['subcategories']\n",
    "\n",
    "print(subcategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subcategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_pub.select(col('summary')).collect()[2]['summary']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = text\n",
    "candidate_labels = categories\n",
    "\n",
    "result = classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(result['scores'][0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, candidate_labels = categories):\n",
    "    result = classifier(text, candidate_labels)\n",
    "    pred_label = result['labels'][0]\n",
    "    score = float(round(result['scores'][0], 4))\n",
    "    return pred_label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the struct return type\n",
    "schema = StructType([\n",
    "    StructField(\"pred_label\", StringType(), True),\n",
    "    StructField(\"score\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_udf = udf(predict, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub_subset = df_pub.limit(10)\n",
    "df_pub_subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "# Define your UDF\n",
    "def predict(text, candidate_labels = categories):\n",
    "    # Check if model has been loaded already\n",
    "    if not globals().get('models_loaded', False):\n",
    "        # globals()['my_model'] = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        globals()['my_model'] = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\")\n",
    "        globals()['models_loaded'] = True\n",
    "\n",
    "    model = globals()['my_model']\n",
    "    result = model(text, candidate_labels)\n",
    "    pred_label = result['labels'][0]\n",
    "    score = float(round(result['scores'][0], 4))\n",
    "    return pred_label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_udf = udf(predict, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions = df_pub_subset.withColumn(\"pred\", predict_udf(col(\"summary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels = df_with_predictions.select(\n",
    "    \"*\",\n",
    "    col(\"pred.pred_label\"),\n",
    "    col(\"pred.score\")\n",
    ").drop(\"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(time, rdd, colname = \"summary\"):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    #df.show()\n",
    "    \n",
    "    # Utilize our predict function\n",
    "    df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "        struct([df[colname])\n",
    "    ))\n",
    "    df_withpreds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions = df_pub_subset.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "candidate_labels = categories\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"predicted_label\", StringType(), True),\n",
    "    StructField(\"confidence_score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "def classify_partition(pdf_iter):\n",
    "    # Only load the model once per Python worker\n",
    "    if not globals().get(\"models_loaded\", False):\n",
    "        globals()[\"classifier\"] = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\", device=-1)\n",
    "        globals()[\"models_loaded\"] = True\n",
    "\n",
    "    classifier = globals()[\"classifier\"]\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        results = []\n",
    "        for text in pdf[\"summary\"]:\n",
    "            res = classifier(text, candidate_labels)\n",
    "            label = res[\"labels\"][0]\n",
    "            score = float(res[\"scores\"][0])\n",
    "            results.append((text, label, score))\n",
    "\n",
    "        yield pd.DataFrame(results, columns=[\"summary\", \"predicted_label\", \"confidence_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame([(\"A new method for protein folding\",)], [\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels = df_pub_subset.mapInPandas(classify_partition, schema=schema)\n",
    "df_with_labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
